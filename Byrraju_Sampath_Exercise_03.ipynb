{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sampath-Varma/Sampath_INFO5731_Fall2024/blob/main/Byrraju_Sampath_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "36ea18c0-20d1-45e1-e292-b814e49da51f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nTask : Topic modelling for news articles \\nUseful features to build the machine learing model :\\n1. Latent Dirichlet Allocation(LDA) topics : It helps in identifying a set of topics in a collection of documents and segregates these topics to each document which in turn it\\n   helps us to find out the structure in the news articles.\\n2. Named Entities : This feature helps us in identifying named entities such as people, organizations, locations and dates which are mentioned in the article.\\n3. Word Embeddings : It represents words in a contionuous vector spcae where it captures semantic similarities betweeen words as the words that are semantically related often cluster\\n   together which helps the model recognize these relationships as it improves classification performance.\\n4. Document Metadata : This features includes date of the publication, author name, or the source of the articles. Metadata can be useful as the articles from specific authors might\\n   be focused particular topics.\\n5. POS Tagging and Key Phrases : Part of Speech tagging identifies nouns, verbs and other granmmatical elements while key phrases identifies important multi words expressions.\\n6. Sentiment Score : It measures the overall emotional tone of the text by analyzing whether the text expresses a positive, negative or neutral sentiment. As different topics have \\n   different sentiment profiles the model can pick up on these differences which helps in distinguishing between different topics that have a similar sentiment.\\n   \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Task : Topic modelling for news articles\n",
        "Useful features to build the machine learing model :\n",
        "1. Latent Dirichlet Allocation(LDA) topics : It helps in identifying a set of topics in a collection of documents and segregates these topics to each document which in turn it\n",
        "   helps us to find out the structure in the news articles.\n",
        "2. Named Entities : This feature helps us in identifying named entities such as people, organizations, locations and dates which are mentioned in the article.\n",
        "3. Word Embeddings : It represents words in a contionuous vector spcae where it captures semantic similarities betweeen words as the words that are semantically related often cluster\n",
        "   together which helps the model recognize these relationships as it improves classification performance.\n",
        "4. Document Metadata : This features includes date of the publication, author name, or the source of the articles. Metadata can be useful as the articles from specific authors might\n",
        "   be focused particular topics.\n",
        "5. POS Tagging and Key Phrases : Part of Speech tagging identifies nouns, verbs and other granmmatical elements while key phrases identifies important multi words expressions.\n",
        "6. Sentiment Score : It measures the overall emotional tone of the text by analyzing whether the text expresses a positive, negative or neutral sentiment. As different topics have\n",
        "   different sentiment profiles the model can pick up on these differences which helps in distinguishing between different topics that have a similar sentiment.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2de551-2e34-40ec-d56d-82199c9239c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting vaderSentiment\n",
            "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting vaderSentiment\n",
            "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "!pip install nltk spacy gensim vaderSentiment sklearn\n",
        "!pip install vaderSentiment\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import gensim\n",
        "import re\n",
        "from gensim import corpora\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from collections import Counter\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "biJgMW4JLu86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6BvnwBlP3VN",
        "outputId": "97a2c6c7-2559-43ee-8763-69409a59ca4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    {\n",
        "        \"text\": \"The government is planning to introduce new policies in the next session of Parliament.\",\n",
        "        \"author\": \"John Doe\",\n",
        "        \"publication_date\": \"2023-05-15\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Apple has announced the launch of its new iPhone in Silicon Valley next month.\",\n",
        "        \"author\": \"Jane Smith\",\n",
        "        \"publication_date\": \"2023-06-01\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The football team won the championship by defeating the defending champions in a thrilling match.\",\n",
        "        \"author\": \"Mike Johnson\",\n",
        "        \"publication_date\": \"2023-07-21\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Scientists have published a new study on climate change, stating that it poses significant risks to the environment.\",\n",
        "        \"author\": \"Emily Clark\",\n",
        "        \"publication_date\": \"2023-08-10\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The stock market saw a significant rise today, driven by the growth of technology companies.\",\n",
        "        \"author\": \"Rachel Lee\",\n",
        "        \"publication_date\": \"2023-09-05\"\n",
        "    }\n",
        "]\n",
        "\n",
        "document_texts = [doc[\"text\"] for doc in documents]"
      ],
      "metadata": {
        "id": "S9lE681cMfU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. LDA Topic Modeling\n",
        "def lda_topic_modeling(docs):\n",
        "    texts = [[word for word in doc.lower().split()] for doc in docs]\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "    topics = lda_model.print_topics(num_words=5)\n",
        "    return topics\n",
        "\n",
        "#2. Named Entities\n",
        "def named_entity_recognition(docs):\n",
        "    entities_list = []\n",
        "    for doc in docs:\n",
        "        doc_nlp = nlp(doc)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc_nlp.ents]\n",
        "        entities_list.append(entities)\n",
        "    return entities_list\n",
        "\n",
        "#3. Word Embeddings\n",
        "def word_embeddings(docs):\n",
        "    doc_embedding_list = []\n",
        "    for doc in docs:\n",
        "        doc_nlp = nlp(doc)\n",
        "        doc_embedding_list.append(doc_nlp.vector)\n",
        "    return doc_embedding_list\n",
        "\n",
        "#4. Document Metadata\n",
        "def extract_metadata(docs):\n",
        "    metadata = []\n",
        "    for doc in docs:\n",
        "        author = doc[\"author\"]\n",
        "        pub_date = datetime.strptime(doc[\"publication_date\"], \"%Y-%m-%d\")\n",
        "        metadata.append({\"author\": author, \"publication_date\": pub_date})\n",
        "    return metadata\n",
        "\n",
        "#5. POS Tagging and Key Phrases\n",
        "def pos_tagging(docs):\n",
        "    pos_tags = []\n",
        "    for doc in docs:\n",
        "        tokens = nltk.word_tokenize(doc)\n",
        "        pos = nltk.pos_tag(tokens)\n",
        "        pos_tags.append(pos)\n",
        "    return pos_tags\n",
        "\n",
        "#6. Sentiment Score using VADER\n",
        "def sentiment_analysis(docs):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = []\n",
        "    for doc in docs:\n",
        "        sentiment = analyzer.polarity_scores(doc)\n",
        "        sentiment_scores.append(sentiment)\n",
        "    return sentiment_scores"
      ],
      "metadata": {
        "id": "nsfiT2hAMzI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. LDA Topic Modeling\n",
        "print(\"\\nLDA Topic Modeling:\")\n",
        "topics = lda_topic_modeling(document_texts)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic {i+1}: {topic}\")\n",
        "\n",
        "#2. Named Entity\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "entities = named_entity_recognition(document_texts)\n",
        "for i, entity in enumerate(entities):\n",
        "    print(f\"Document {i+1}: {entity}\")\n",
        "\n",
        "#3. Word Embeddings\n",
        "print(\"\\nWord Embeddings:\")\n",
        "embeddings = word_embeddings(document_texts)\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    print(f\"Document {i+1} Embedding Shape: {embedding.shape}\")\n",
        "\n",
        "#4. Document Metadata\n",
        "print(\"\\nDocument Metadata:\")\n",
        "metadata = extract_metadata(documents)\n",
        "for i, meta in enumerate(metadata):\n",
        "    print(f\"Document {i+1} Metadata: Author - {meta['author']}, Publication Date - {meta['publication_date']}\")\n",
        "\n",
        "#5. Sentiment Analysis\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "sentiment_scores = sentiment_analysis(document_texts)\n",
        "for i, score in enumerate(sentiment_scores):\n",
        "    print(f\"Document {i+1} Sentiment: {score}\")\n",
        "\n",
        "\n",
        "#6. POS Tagging\n",
        "print(\"\\nPOS Tagging:\")\n",
        "pos_tags = pos_tagging(document_texts)\n",
        "for i, pos in enumerate(pos_tags):\n",
        "    print(f\"Document {i+1} POS Tags: {pos[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxvYdFsyM6I7",
        "outputId": "494b4106-9d4c-432e-96a3-791869d6a00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA Topic Modeling:\n",
            "Topic 1: (0, '0.091*\"the\" + 0.049*\"in\" + 0.028*\"next\" + 0.028*\"by\" + 0.028*\"defending\"')\n",
            "Topic 2: (1, '0.081*\"the\" + 0.036*\"of\" + 0.036*\"new\" + 0.036*\"a\" + 0.036*\"significant\"')\n",
            "Topic 3: (2, '0.018*\"the\" + 0.018*\"a\" + 0.018*\"new\" + 0.018*\"of\" + 0.018*\"to\"')\n",
            "\n",
            "Named Entity Recognition:\n",
            "Document 1: [('Parliament', 'ORG')]\n",
            "Document 2: [('Apple', 'ORG'), ('Silicon Valley', 'LOC'), ('next month', 'DATE')]\n",
            "Document 3: []\n",
            "Document 4: []\n",
            "Document 5: [('today', 'DATE')]\n",
            "\n",
            "Word Embeddings:\n",
            "Document 1 Embedding Shape: (96,)\n",
            "Document 2 Embedding Shape: (96,)\n",
            "Document 3 Embedding Shape: (96,)\n",
            "Document 4 Embedding Shape: (96,)\n",
            "Document 5 Embedding Shape: (96,)\n",
            "\n",
            "Document Metadata:\n",
            "Document 1 Metadata: Author - John Doe, Publication Date - 2023-05-15 00:00:00\n",
            "Document 2 Metadata: Author - Jane Smith, Publication Date - 2023-06-01 00:00:00\n",
            "Document 3 Metadata: Author - Mike Johnson, Publication Date - 2023-07-21 00:00:00\n",
            "Document 4 Metadata: Author - Emily Clark, Publication Date - 2023-08-10 00:00:00\n",
            "Document 5 Metadata: Author - Rachel Lee, Publication Date - 2023-09-05 00:00:00\n",
            "\n",
            "Sentiment Analysis:\n",
            "Document 1 Sentiment: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Document 2 Sentiment: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Document 3 Sentiment: {'neg': 0.101, 'neu': 0.389, 'pos': 0.51, 'compound': 0.8885}\n",
            "Document 4 Sentiment: {'neg': 0.106, 'neu': 0.804, 'pos': 0.09, 'compound': -0.0772}\n",
            "Document 5 Sentiment: {'neg': 0.0, 'neu': 0.747, 'pos': 0.253, 'compound': 0.5267}\n",
            "\n",
            "POS Tagging:\n",
            "Document 1 POS Tags: [('The', 'DT'), ('government', 'NN'), ('is', 'VBZ'), ('planning', 'VBG'), ('to', 'TO')]\n",
            "Document 2 POS Tags: [('Apple', 'NNP'), ('has', 'VBZ'), ('announced', 'VBN'), ('the', 'DT'), ('launch', 'NN')]\n",
            "Document 3 POS Tags: [('The', 'DT'), ('football', 'NN'), ('team', 'NN'), ('won', 'VBD'), ('the', 'DT')]\n",
            "Document 4 POS Tags: [('Scientists', 'NNS'), ('have', 'VBP'), ('published', 'VBN'), ('a', 'DT'), ('new', 'JJ')]\n",
            "Document 5 POS Tags: [('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('saw', 'VBD'), ('a', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022b645b-dc0e-4de9-daf8-8766aaef3a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('today', 4.000000000000001),\n",
              " ('rise', 4.000000000000001),\n",
              " ('on', 4.000000000000001),\n",
              " ('parliament', 4.000000000000001),\n",
              " ('planning', 4.000000000000001),\n",
              " ('policies', 4.000000000000001),\n",
              " ('published', 4.000000000000001),\n",
              " ('poses', 4.000000000000001),\n",
              " ('risks', 4.000000000000001),\n",
              " ('scientists', 4.000000000000001),\n",
              " ('team', 4.000000000000001),\n",
              " ('thrilling', 4.000000000000001),\n",
              " ('to', 3.0),\n",
              " ('significant', 3.0),\n",
              " ('next', 3.0),\n",
              " ('by', 3.0),\n",
              " ('of', 1.9999999999999996),\n",
              " ('new', 1.9999999999999996),\n",
              " ('in', 1.9999999999999996),\n",
              " ('the', 1.5555555555555556)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#I have used the Chi-Squared feature selection method.\n",
        "# You code here (Please add comments in the code):\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample text data with metadata (author and publication date)\n",
        "docs = [\n",
        "    {\n",
        "        \"text\": \"The government is planning to introduce new policies in the next session of Parliament.\",\n",
        "        \"author\": \"John Doe\",\n",
        "        \"publication_date\": \"2023-05-15\",\n",
        "        \"label\": \"politics\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Apple has announced the launch of its new iPhone in Silicon Valley next month.\",\n",
        "        \"author\": \"Jane Smith\",\n",
        "        \"publication_date\": \"2023-06-01\",\n",
        "        \"label\": \"business\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The football team won the championship by defeating the defending champions in a thrilling match.\",\n",
        "        \"author\": \"Mike Johnson\",\n",
        "        \"publication_date\": \"2023-07-21\",\n",
        "        \"label\": \"sports\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Scientists have published a new study on climate change, stating that it poses significant risks to the environment.\",\n",
        "        \"author\": \"Emily Clark\",\n",
        "        \"publication_date\": \"2023-08-10\",\n",
        "        \"label\": \"science\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The stock market saw a significant rise today, driven by the growth of technology companies.\",\n",
        "        \"author\": \"Rachel Lee\",\n",
        "        \"publication_date\": \"2023-09-05\",\n",
        "        \"label\": \"finance\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Step 1: Extract the text and labels\n",
        "texts = [doc[\"text\"] for doc in docs]\n",
        "labels = [doc[\"label\"] for doc in docs]\n",
        "\n",
        "# Step 2: Convert the text into raw word counts using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=20)  # Limit to top 20 words\n",
        "X_counts = vectorizer.fit_transform(texts).toarray()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Step 3: Encode the labels into numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Step 4: Compute Chi-Square Statistic\n",
        "chi2_scores, _ = chi2(X_counts, y_encoded)\n",
        "\n",
        "# Step 5: Rank features based on Chi-Square scores\n",
        "sorted_indices = np.argsort(chi2_scores)[::-1]\n",
        "ranked_features = [(feature_names[i], chi2_scores[i]) for i in sorted_indices]\n",
        "\n",
        "# Display ranked features\n",
        "ranked_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fb0f77-2c78-4b5f-aabc-9500ca59cfa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "!pip install transformers torch scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data from question 2\n",
        "docs = [\n",
        "    {\n",
        "        \"text\": \"The government is planning to introduce new policies in the next session of Parliament.\",\n",
        "        \"author\": \"John Doe\",\n",
        "        \"publication_date\": \"2023-05-15\",\n",
        "        \"label\": \"politics\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Apple has announced the launch of its new iPhone in Silicon Valley next month.\",\n",
        "        \"author\": \"Jane Smith\",\n",
        "        \"publication_date\": \"2023-06-01\",\n",
        "        \"label\": \"business\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The football team won the championship by defeating the defending champions in a thrilling match.\",\n",
        "        \"author\": \"Mike Johnson\",\n",
        "        \"publication_date\": \"2023-07-21\",\n",
        "        \"label\": \"sports\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Scientists have published a new study on climate change, stating that it poses significant risks to the environment.\",\n",
        "        \"author\": \"Emily Clark\",\n",
        "        \"publication_date\": \"2023-08-10\",\n",
        "        \"label\": \"science\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The stock market saw a significant rise today, driven by the growth of technology companies.\",\n",
        "        \"author\": \"Rachel Lee\",\n",
        "        \"publication_date\": \"2023-09-05\",\n",
        "        \"label\": \"finance\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Query to match\n",
        "query = \"What are the latest policies introduced by the government?\"\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# Get embeddings for the documents\n",
        "doc_embeddings = [get_embeddings(doc[\"text\"]) for doc in docs]\n",
        "doc_embeddings = np.vstack(doc_embeddings)\n",
        "\n",
        "# Get embedding for the query\n",
        "query_embedding = get_embeddings(query)\n",
        "\n",
        "# Calculating cosine similarity\n",
        "similarity_scores = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
        "\n",
        "# Rank documents based on similarity scores in descending order\n",
        "ranked_indices = np.argsort(similarity_scores)[::-1]\n",
        "\n",
        "# Display ranked documents with their similarity scores\n",
        "ranked_docs = [(docs[i][\"text\"], similarity_scores[i]) for i in ranked_indices]\n",
        "\n",
        "# Print the results\n",
        "for text, score in ranked_docs:\n",
        "    print(f\"Document: {text}\\nSimilarity Score: {score:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nkolJ1ak9XV",
        "outputId": "5f675748-f5b6-4b2f-c720-9013722bdbc8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: The government is planning to introduce new policies in the next session of Parliament.\n",
            "Similarity Score: 0.8489\n",
            "\n",
            "Document: Scientists have published a new study on climate change, stating that it poses significant risks to the environment.\n",
            "Similarity Score: 0.8489\n",
            "\n",
            "Document: Apple has announced the launch of its new iPhone in Silicon Valley next month.\n",
            "Similarity Score: 0.8043\n",
            "\n",
            "Document: The stock market saw a significant rise today, driven by the growth of technology companies.\n",
            "Similarity Score: 0.8016\n",
            "\n",
            "Document: The football team won the championship by defeating the defending champions in a thrilling match.\n",
            "Similarity Score: 0.7989\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Learning Experience :\n",
        "The first question was a good challenege where i had taken a use case for text classification where i had choose the task as \"Topic modelling for news articles\" and have explained\n",
        "some of the useful features such as latent dirichlet allocation, word embeddings, document metadata and POS tagging and key phrases.I learned about various methods for extracting\n",
        "features from text, such as Bag of Words, TF-IDF, and word embeddings (e.g., using BERT). Understanding how these methods convert raw text into numerical representations was crucial.\n",
        "Learning to calculate cosine similarity using embeddings from BERT provided practical insight into comparing the semantic similarity of text.Working on extracting features from text\n",
        "data was an enriching experience that enhanced my understanding of Natural Language Processing.\n",
        "Challenges Encountered :\n",
        "There were some challeneges in the assignment but particularly understanig how BERT embeddings represent text and how to extract meaningful information from them took time.\n",
        "Relevance to my field of study :\n",
        "This exercise where i have learnt about feature extraction which is essential for tasks in natural language processing (NLP) like classifying text, analyzing feelings, and finding\n",
        "information. Knowing how to represent text so that it conveys its meaning is important for creating strong NLP applications.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "98e0e9a7-98a9-4385-9bf3-48d67b9f8f2c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nLearning Experience :\\nThe first question was a good challenege where i had taken a use case for text classification where i had choose the task as \"Topic modelling for news articles\" and have explained\\nsome of the useful features such as latent dirichlet allocation, word embeddings, document metadata and POS tagging and key phrases.I learned about various methods for extracting\\nfeatures from text, such as Bag of Words, TF-IDF, and word embeddings (e.g., using BERT). Understanding how these methods convert raw text into numerical representations was crucial.\\nLearning to calculate cosine similarity using embeddings from BERT provided practical insight into comparing the semantic similarity of text.Working on extracting features from text \\ndata was an enriching experience that enhanced my understanding of Natural Language Processing.\\nChallenges Encountered :\\nThere were some challeneges in the assignment but particularly understanig how BERT embeddings represent text and how to extract meaningful information from them took time.\\nRelevance to my field of study :\\nThis exercise where i have learnt about feature extraction which is essential for tasks in natural language processing (NLP) like classifying text, analyzing feelings, and finding \\ninformation. Knowing how to represent text so that it conveys its meaning is important for creating strong NLP applications.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}