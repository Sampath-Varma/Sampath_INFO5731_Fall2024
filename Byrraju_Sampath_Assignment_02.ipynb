{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sampath-Varma/Sampath_INFO5731_Fall2024/blob/main/Byrraju_Sampath_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641d566a-8b47-44c6-9d92-1d80b660c061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_reviews(imdb_id, num_reviews=1000):\n",
        "    reviews = []\n",
        "    page = 0\n",
        "    base_url = f\"https://www.imdb.com/title/{imdb_id}/reviews?ref_=tt_ql_3\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        page += 1\n",
        "        print(f\"Scraping page {page} of reviews:\")\n",
        "\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        if response.status_code == 403:\n",
        "            print(\"Access denied (403).\")\n",
        "            break\n",
        "        elif response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        review_containers = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        if not review_containers:\n",
        "            print(\"No more reviews found\")\n",
        "            break\n",
        "\n",
        "        for container in review_containers:\n",
        "            if len(reviews) >= num_reviews:\n",
        "                break\n",
        "            reviews.append(container.get_text(strip=True))\n",
        "\n",
        "        print(f\"Collected {len(reviews)} reviews so far\")\n",
        "\n",
        "        load_more_button = soup.find('div', class_='load-more-data')\n",
        "        if load_more_button and len(reviews) < num_reviews:\n",
        "            next_page_key = load_more_button.get('data-key')\n",
        "            base_url = f\"https://www.imdb.com/title/{imdb_id}/reviews/_ajax?ref_=undefined&paginationKey={next_page_key}\"\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "def save_reviews_to_csv(movie_title, movie_reviews, filename='imdb_reviews.csv'):\n",
        "    df = pd.DataFrame(movie_reviews, columns=['Review'])\n",
        "    df['Movie Title'] = movie_title\n",
        "    df = df[['Movie Title', 'Review']]\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {len(df)} reviews to {filename}\")\n",
        "\n",
        "def collect_movie_reviews():\n",
        "\n",
        "    movie = {\"title\": \"Dune: Part Two\", \"id\": \"tt15239678\"}\n",
        "\n",
        "    print(f\"Scraping reviews for {movie['title']}:\")\n",
        "    reviews = scrape_reviews(movie['id'])\n",
        "\n",
        "    if len(reviews) > 0:\n",
        "        save_reviews_to_csv(movie['title'], reviews)\n",
        "    else:\n",
        "        print(\"No reviews collected\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    collect_movie_reviews()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odTT41AyrJ6e",
        "outputId": "3856f917-6ff0-4e03-8455-0a473c04e50a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping reviews for Dune: Part Two:\n",
            "Scraping page 1 of reviews:\n",
            "Collected 25 reviews so far\n",
            "Scraping page 2 of reviews:\n",
            "Collected 50 reviews so far\n",
            "Scraping page 3 of reviews:\n",
            "Collected 75 reviews so far\n",
            "Scraping page 4 of reviews:\n",
            "Collected 100 reviews so far\n",
            "Scraping page 5 of reviews:\n",
            "Collected 125 reviews so far\n",
            "Scraping page 6 of reviews:\n",
            "Collected 150 reviews so far\n",
            "Scraping page 7 of reviews:\n",
            "Collected 175 reviews so far\n",
            "Scraping page 8 of reviews:\n",
            "Collected 200 reviews so far\n",
            "Scraping page 9 of reviews:\n",
            "Collected 225 reviews so far\n",
            "Scraping page 10 of reviews:\n",
            "Collected 250 reviews so far\n",
            "Scraping page 11 of reviews:\n",
            "Collected 275 reviews so far\n",
            "Scraping page 12 of reviews:\n",
            "Collected 300 reviews so far\n",
            "Scraping page 13 of reviews:\n",
            "Collected 325 reviews so far\n",
            "Scraping page 14 of reviews:\n",
            "Collected 350 reviews so far\n",
            "Scraping page 15 of reviews:\n",
            "Collected 375 reviews so far\n",
            "Scraping page 16 of reviews:\n",
            "Collected 400 reviews so far\n",
            "Scraping page 17 of reviews:\n",
            "Collected 425 reviews so far\n",
            "Scraping page 18 of reviews:\n",
            "Collected 450 reviews so far\n",
            "Scraping page 19 of reviews:\n",
            "Collected 475 reviews so far\n",
            "Scraping page 20 of reviews:\n",
            "Collected 500 reviews so far\n",
            "Scraping page 21 of reviews:\n",
            "Collected 525 reviews so far\n",
            "Scraping page 22 of reviews:\n",
            "Collected 550 reviews so far\n",
            "Scraping page 23 of reviews:\n",
            "Collected 575 reviews so far\n",
            "Scraping page 24 of reviews:\n",
            "Collected 600 reviews so far\n",
            "Scraping page 25 of reviews:\n",
            "Collected 625 reviews so far\n",
            "Scraping page 26 of reviews:\n",
            "Collected 650 reviews so far\n",
            "Scraping page 27 of reviews:\n",
            "Collected 675 reviews so far\n",
            "Scraping page 28 of reviews:\n",
            "Collected 700 reviews so far\n",
            "Scraping page 29 of reviews:\n",
            "Collected 725 reviews so far\n",
            "Scraping page 30 of reviews:\n",
            "Collected 750 reviews so far\n",
            "Scraping page 31 of reviews:\n",
            "Collected 775 reviews so far\n",
            "Scraping page 32 of reviews:\n",
            "Collected 800 reviews so far\n",
            "Scraping page 33 of reviews:\n",
            "Collected 825 reviews so far\n",
            "Scraping page 34 of reviews:\n",
            "Collected 850 reviews so far\n",
            "Scraping page 35 of reviews:\n",
            "Collected 875 reviews so far\n",
            "Scraping page 36 of reviews:\n",
            "Collected 900 reviews so far\n",
            "Scraping page 37 of reviews:\n",
            "Collected 925 reviews so far\n",
            "Scraping page 38 of reviews:\n",
            "Collected 950 reviews so far\n",
            "Scraping page 39 of reviews:\n",
            "Collected 975 reviews so far\n",
            "Scraping page 40 of reviews:\n",
            "Collected 1000 reviews so far\n",
            "Saved 1000 reviews to imdb_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "file_path = '/content/imdb_reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Initial Dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "#1. Remove noise such as special characters and punctuations\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    return text\n",
        "df['cleaned_review'] = df['Review'].apply(remove_noise)\n",
        "print(\"After Removing Noise:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "#2. Remove numbers\n",
        "def remove_numbers(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(remove_numbers)\n",
        "print(\"After Removing Numbers:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "#3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(remove_stopwords)\n",
        "print(\"After Removing Stopwords:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "#4. Lowercase all texts\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(lambda text: text.lower())\n",
        "print(\"After Lowercasing:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "#5. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "def stemming(text):\n",
        "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(stemming)\n",
        "print(\"After Stemming:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "#6. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(lemmatization)\n",
        "print(\"After Lemmatization:\")\n",
        "print(df[['Review', 'cleaned_review']].head(), \"\\n\")\n",
        "\n",
        "output_file_path = 'imdb_reviews_cleaned.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"Cleaned data saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n80Pywt9Mix9",
        "outputId": "76ea89e6-2a0e-4690-8290-9e497aa5e3c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Dataset:\n",
            "      Movie Title                                             Review\n",
            "0  Dune: Part Two  This is what Hollywood needs. A great story wi...\n",
            "1  Dune: Part Two  I'm going to write this as a review for both D...\n",
            "2  Dune: Part Two  Had the pleasure to watch this film in an earl...\n",
            "3  Dune: Part Two  Phenomenal stuff. I'll probably calm down tomo...\n",
            "4  Dune: Part Two  If you liked or loved the first one, the same ... \n",
            "\n",
            "After Removing Noise:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  This is what Hollywood needs A great story wit...  \n",
            "1  Im going to write this as a review for both Du...  \n",
            "2  Had the pleasure to watch this film in an earl...  \n",
            "3  Phenomenal stuff Ill probably calm down tomorr...  \n",
            "4  If you liked or loved the first one the same w...   \n",
            "\n",
            "After Removing Numbers:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  This is what Hollywood needs A great story wit...  \n",
            "1  Im going to write this as a review for both Du...  \n",
            "2  Had the pleasure to watch this film in an earl...  \n",
            "3  Phenomenal stuff Ill probably calm down tomorr...  \n",
            "4  If you liked or loved the first one the same w...   \n",
            "\n",
            "After Removing Stopwords:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  Hollywood needs great story great directorprod...  \n",
            "1  Im going write review Dune movies Ill include ...  \n",
            "2  pleasure watch film early screening completely...  \n",
            "3  Phenomenal stuff Ill probably calm tomorrow ri...  \n",
            "4  liked loved first one apply one Personally lov...   \n",
            "\n",
            "After Lowercasing:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  hollywood needs great story great directorprod...  \n",
            "1  im going write review dune movies ill include ...  \n",
            "2  pleasure watch film early screening completely...  \n",
            "3  phenomenal stuff ill probably calm tomorrow ri...  \n",
            "4  liked loved first one apply one personally lov...   \n",
            "\n",
            "After Stemming:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  hollywood need great stori great directorprodu...  \n",
            "1  im go write review dune movi ill includ though...  \n",
            "2  pleasur watch film earli screen complet blown ...  \n",
            "3  phenomen stuff ill probabl calm tomorrow right...  \n",
            "4  like love first one appli one person love one ...   \n",
            "\n",
            "After Lemmatization:\n",
            "                                              Review  \\\n",
            "0  This is what Hollywood needs. A great story wi...   \n",
            "1  I'm going to write this as a review for both D...   \n",
            "2  Had the pleasure to watch this film in an earl...   \n",
            "3  Phenomenal stuff. I'll probably calm down tomo...   \n",
            "4  If you liked or loved the first one, the same ...   \n",
            "\n",
            "                                      cleaned_review  \n",
            "0  hollywood need great stori great directorprodu...  \n",
            "1  im go write review dune movi ill includ though...  \n",
            "2  pleasur watch film earli screen complet blown ...  \n",
            "3  phenomen stuff ill probabl calm tomorrow right...  \n",
            "4  like love first one appli one person love one ...   \n",
            "\n",
            "Cleaned data saved to imdb_reviews_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c234356-578f-4627-bfd4-d3c2d989afc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "file_path = '/content/imdb_reviews_cleaned.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#(1) Parts of Speech (POS) Tagging\n",
        "def pos_analysis(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "df['pos_tags'] = df['cleaned_review'].apply(pos_analysis)\n",
        "pos_counts = Counter(tag for tags in df['pos_tags'] for _, tag in tags)\n",
        "print(\"Parts of Speech Counts:\")\n",
        "print(f\"Nouns: {pos_counts['NN'] + pos_counts['NNS'] + pos_counts['NNP'] + pos_counts['NNPS']}\")\n",
        "print(f\"Verbs: {pos_counts['VB'] + pos_counts['VBD'] + pos_counts['VBG'] + pos_counts['VBN'] + pos_counts['VBP'] + pos_counts['VBZ']}\")\n",
        "print(f\"Adjectives: {pos_counts['JJ'] + pos_counts['JJR'] + pos_counts['JJS']}\")\n",
        "print(f\"Adverbs: {pos_counts['RB'] + pos_counts['RBR'] + pos_counts['RBS']}\\n\")\n",
        "\n",
        "#(2) Dependency Parsing\n",
        "def parse_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    for sent in doc.sents:\n",
        "        print(f\"Sentence: {sent.text}\")\n",
        "        print(\"Dependency Parse:\")\n",
        "        for token in sent:\n",
        "            print(f\"{token.text:10} > {token.dep_:15} ({token.head.text})\")\n",
        "        print(\"\\n\")\n",
        "example_sentence = df['cleaned_review'].iloc[0]\n",
        "print(\"Example Analysis for the First Review Sentence:\")\n",
        "parse_sentences(example_sentence)\n",
        "\n",
        "#(3) Named Entity Recognition (NER)\n",
        "def ner_analysis(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "df['entities'] = df['cleaned_review'].apply(ner_analysis)\n",
        "entity_counts = Counter(ent[1] for entities in df['entities'] for ent in entities)\n",
        "print(\"Named Entity Recognition Counts:\")\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct0kvfuHVtJJ",
        "outputId": "8e74d5c0-0638-4ae0-aedc-975797689f28"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech Counts:\n",
            "Nouns: 63338\n",
            "Verbs: 14669\n",
            "Adjectives: 23244\n",
            "Adverbs: 5396\n",
            "\n",
            "Example Analysis for the First Review Sentence:\n",
            "Sentence: hollywood need great stori great directorproduc best thing studio get hell way let artist creat artdun part creativ beauti tragic mesmer never get bore anticip go happen next havent read book noth compar todeni villeneuv continu amaz effort put film act film top notch saw imax sound earth shatter your gonna see movi see largest screen possibl\n",
            "Dependency Parse:\n",
            "hollywood  > nsubj           (need)\n",
            "need       > ROOT            (need)\n",
            "great      > amod            (stori)\n",
            "stori      > nmod            (studio)\n",
            "great      > amod            (studio)\n",
            "directorproduc > npadvmod        (best)\n",
            "best       > amod            (thing)\n",
            "thing      > compound        (studio)\n",
            "studio     > nsubj           (get)\n",
            "get        > ccomp           (need)\n",
            "hell       > compound        (way)\n",
            "way        > npadvmod        (let)\n",
            "let        > advcl           (need)\n",
            "artist     > compound        (creat)\n",
            "creat      > nsubj           (artdun)\n",
            "artdun     > ccomp           (let)\n",
            "part       > compound        (beauti)\n",
            "creativ    > compound        (beauti)\n",
            "beauti     > dobj            (artdun)\n",
            "tragic     > amod            (mesmer)\n",
            "mesmer     > nsubj           (get)\n",
            "never      > neg             (get)\n",
            "get        > ccomp           (let)\n",
            "bore       > compound        (anticip)\n",
            "anticip    > nsubj           (go)\n",
            "go         > ccomp           (get)\n",
            "happen     > advcl           (go)\n",
            "next       > advmod          (happen)\n",
            "have       > aux             (read)\n",
            "nt         > neg             (read)\n",
            "read       > conj            (get)\n",
            "book       > compound        (todeni)\n",
            "noth       > compound        (compar)\n",
            "compar     > compound        (todeni)\n",
            "todeni     > dobj            (read)\n",
            "villeneuv  > compound        (continu)\n",
            "continu    > nsubj           (put)\n",
            "amaz       > advmod          (effort)\n",
            "effort     > nsubj           (put)\n",
            "put        > advcl           (need)\n",
            "film       > compound        (notch)\n",
            "act        > compound        (notch)\n",
            "film       > nmod            (notch)\n",
            "top        > amod            (notch)\n",
            "notch      > dobj            (put)\n",
            "saw        > conj            (need)\n",
            "imax       > amod            (shatter)\n",
            "sound      > amod            (shatter)\n",
            "earth      > compound        (shatter)\n",
            "shatter    > nsubj           (gon)\n",
            "your       > poss            (gon)\n",
            "gon        > ccomp           (saw)\n",
            "na         > aux             (see)\n",
            "see        > xcomp           (gon)\n",
            "movi       > nsubj           (see)\n",
            "see        > ccomp           (see)\n",
            "largest    > amod            (possibl)\n",
            "screen     > compound        (possibl)\n",
            "possibl    > dobj            (see)\n",
            "\n",
            "\n",
            "Named Entity Recognition Counts:\n",
            "Counter({'PERSON': 3182, 'CARDINAL': 1397, 'ORDINAL': 997, 'ORG': 941, 'NORP': 426, 'DATE': 274, 'GPE': 269, 'TIME': 168, 'PRODUCT': 67, 'FAC': 17, 'WORK_OF_ART': 15, 'QUANTITY': 7, 'LOC': 7, 'EVENT': 5, 'LANGUAGE': 3, 'MONEY': 2, 'LAW': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''\n",
        "In the first question i have choosen the 2nd task which is to collect the top 1000 user reviews of Dune Part : 2 from IMDB which involved performing scrapping reviews where i have\n",
        "used beautifulsoup and requets with appropriate headers to mimic a browser. I have implemented paginatio  to collect multiple pages of reviews untill i have reached 1000 reviews\n",
        "and saved the 1000 reviews into a csv file. In the second question using the obtained dataset i had removed noise by eliminating special characters and punctuation form the text,\n",
        "removed numbers, lowercased the text and also removed common words and applied stemming and lemmatization. These results were saved in a new csv file. Finally in the third question\n",
        "I have conducted a syntax and structure analysis on the cleaned dataset which inlcudes POS tagging, parsing and NER. The main challenge i encountered was in the 1st question where I\n",
        "couldn't get to scrape data due to access restrictions and even after that there was diffuculty of not getting all the 1000 reviews but i really enjoyed this process of scrapping the\n",
        "reviews. I think the provided time for this assignment is sufficient to complete it.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "86696894-4e18-499a-8c27-e980c976ecae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nIn the first question i have choosen the 2nd task which is to collect the top 1000 user reviews of Dune Part : 2 from IMDB which involved performing scrapping reviews where i have \\nused beautifulsoup and requets with appropriate headers to mimic a browser. I have implemented paginatio  to collect multiple pages of reviews untill i have reached 1000 reviews \\nand saved the 1000 reviews into a csv file. In the second question using the obtained dataset i had removed noise by eliminating special characters and punctuation form the text, \\nremoved numbers, lowercased the text and also removed common words and applied stemming and lemmatization. These results were saved in a new csv file. Finally in the third question\\nI have conducted a syntax and structure analysis on the cleaned dataset which inlcudes POS tagging, parsing and NER. The main challenge i encountered was in the 1st question where I\\ncouldn't get to scrape data due to access restrictions and even after that there was diffuculty of not getting all the 1000 reviews but i really enjoyed this process of scrapping the\\nreviews. I think the provided time for this assignment is sufficient to complete it.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}